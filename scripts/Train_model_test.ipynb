{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model and do test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from Hyperparameter_optimization import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "\n",
    "## Load the training set\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y_train, tX_train, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "feature_names = load_headers(DATA_TRAIN_PATH)\n",
    "\n",
    "# convert y from -1/1 to 0,1\n",
    "y_train = (y_train+1)/2\n",
    "\n",
    "## Load the test set\n",
    "\n",
    "# Load test set\n",
    "DATA_TRAIN_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "feature_names = load_headers(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a model and see accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌──────────────────────────────────────┐\n",
      "│               Group: 0               │\n",
      "└──────────────────────────────────────┘\n",
      "────────────────────────────────────────\n",
      "lambda : 0.001,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.06845308622720489 \n",
      " Accuracy: 0.8099606992817454\n",
      "────────────────────────────────────────\n",
      "lambda : 0.000945,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.06844933827073421 \n",
      " Accuracy: 0.8099200433663098\n",
      "────────────────────────────────────────\n",
      "lambda : 0.0008900000000000001,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.06844565462037186 \n",
      " Accuracy: 0.8099335953381217\n",
      "────────────────────────────────────────\n",
      "lambda : 0.000835,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.06844202545693502 \n",
      " Accuracy: 0.8100420111126168\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00078,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.06843847600374693 \n",
      " Accuracy: 0.8100826670280525\n",
      "────────────────────────────────────────\n",
      "lambda : 0.000725,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.06843499266567252 \n",
      " Accuracy: 0.8100826670280525\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00067,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.06843160486269903 \n",
      " Accuracy: 0.8101233229434882\n",
      "────────────────────────────────────────\n",
      "lambda : 0.000615,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.06842828300105466 \n",
      " Accuracy: 0.8101639788589239\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00056,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.06842505980079229 \n",
      " Accuracy: 0.8101368749153002\n",
      "────────────────────────────────────────\n",
      "lambda : 0.000505,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.068421928824572 \n",
      " Accuracy: 0.8101775308307358\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00045,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.06841888897008633 \n",
      " Accuracy: 0.8102452906897953\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00039499999999999995,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.06841596461038775 \n",
      " Accuracy: 0.8102588426616073\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00034,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.06841314642502758 \n",
      " Accuracy: 0.8102317387179834\n",
      "────────────────────────────────────────\n",
      "lambda : 0.000285,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.06841043079068956 \n",
      " Accuracy: 0.810299498577043\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00022999999999999995,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.068407788007086 \n",
      " Accuracy: 0.8103808104079142\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00017500000000000003,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.06840516628211489 \n",
      " Accuracy: 0.8103943623797264\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00011999999999999999,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.06840239535814917 \n",
      " Accuracy: 0.8104214663233501\n",
      "────────────────────────────────────────\n",
      "lambda : 6.499999999999995e-05,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.06839884439618205 \n",
      " Accuracy: 0.8105027781542213\n",
      "────────────────────────────────────────\n",
      "lambda : 1e-05,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.06839048569825962 \n",
      " Accuracy: 0.8104350182951621\n",
      "════════════════════════════════════════\n",
      "*The optimal hyperparameters*:\n",
      " Model: ridge_regression \n",
      " Optimal accuracy: 0.8105027781542213 \n",
      " Optimal lambda_: 6.499999999999995e-05 \n",
      " Optimal gamma: 0.01 \n",
      " \n",
      "┌──────────────────────────────────────┐\n",
      "│               Group: 1               │\n",
      "└──────────────────────────────────────┘\n",
      "────────────────────────────────────────\n",
      "lambda : 0.001,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.020415266543190428 \n",
      " Accuracy: 0.9492343032159265\n",
      "────────────────────────────────────────\n",
      "lambda : 0.000945,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.020414266835776428 \n",
      " Accuracy: 0.9492343032159265\n",
      "────────────────────────────────────────\n",
      "lambda : 0.0008900000000000001,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.020413244165672285 \n",
      " Accuracy: 0.9492343032159265\n",
      "────────────────────────────────────────\n",
      "lambda : 0.000835,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.02041222415270137 \n",
      " Accuracy: 0.9492725880551302\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00078,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.02041114412339235 \n",
      " Accuracy: 0.9492725880551302\n",
      "────────────────────────────────────────\n",
      "lambda : 0.000725,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.020410076232531403 \n",
      " Accuracy: 0.9492725880551302\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00067,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.020408935743339646 \n",
      " Accuracy: 0.9493108728943339\n",
      "────────────────────────────────────────\n",
      "lambda : 0.000615,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.020407732574824942 \n",
      " Accuracy: 0.9492725880551302\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00056,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.02040657860341944 \n",
      " Accuracy: 0.9492343032159265\n",
      "────────────────────────────────────────\n",
      "lambda : 0.000505,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.020405247076847162 \n",
      " Accuracy: 0.9493108728943339\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00045,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.02042684740704332 \n",
      " Accuracy: 0.9492343032159265\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00039499999999999995,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.020426411109974507 \n",
      " Accuracy: 0.9492725880551302\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00034,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.020402612525001182 \n",
      " Accuracy: 0.9493874425727412\n",
      "────────────────────────────────────────\n",
      "lambda : 0.000285,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.020400371743301345 \n",
      " Accuracy: 0.9492725880551302\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00022999999999999995,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.02039885366368007 \n",
      " Accuracy: 0.9493108728943339\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00017500000000000003,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.0203972209125983 \n",
      " Accuracy: 0.9493874425727412\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00011999999999999999,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.020400935803085142 \n",
      " Accuracy: 0.9494257274119449\n",
      "────────────────────────────────────────\n",
      "lambda : 6.499999999999995e-05,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.0204058107453265 \n",
      " Accuracy: 0.9495022970903522\n",
      "────────────────────────────────────────\n",
      "lambda : 1e-05,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.02038796846679237 \n",
      " Accuracy: 0.9495405819295559\n",
      "════════════════════════════════════════\n",
      "*The optimal hyperparameters*:\n",
      " Model: ridge_regression \n",
      " Optimal accuracy: 0.9495405819295559 \n",
      " Optimal lambda_: 1e-05 \n",
      " Optimal gamma: 0.01 \n",
      " \n",
      "┌──────────────────────────────────────┐\n",
      "│               Group: 2               │\n",
      "└──────────────────────────────────────┘\n",
      "────────────────────────────────────────\n",
      "lambda : 0.001,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.07297005057941157 \n",
      " Accuracy: 0.7982995141468991\n",
      "────────────────────────────────────────\n",
      "lambda : 0.000945,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.07296202730016514 \n",
      " Accuracy: 0.7984852815090026\n",
      "────────────────────────────────────────\n",
      "lambda : 0.0008900000000000001,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.07295417723244149 \n",
      " Accuracy: 0.7985138611031724\n",
      "────────────────────────────────────────\n",
      "lambda : 0.000835,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.07294650927439035 \n",
      " Accuracy: 0.7983852529294084\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00078,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.07293903363203866 \n",
      " Accuracy: 0.7985138611031722\n",
      "────────────────────────────────────────\n",
      "lambda : 0.000725,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.07293176061086822 \n",
      " Accuracy: 0.7984852815090024\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00067,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.07292470169482088 \n",
      " Accuracy: 0.7983709631323235\n",
      "────────────────────────────────────────\n",
      "lambda : 0.000615,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.07291786896519772 \n",
      " Accuracy: 0.7984709917119177\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00056,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.07291127574609821 \n",
      " Accuracy: 0.7984852815090026\n",
      "────────────────────────────────────────\n",
      "lambda : 0.000505,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.0729049364077581 \n",
      " Accuracy: 0.7983852529294083\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00045,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.0728988669923485 \n",
      " Accuracy: 0.7983280937410688\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00039499999999999995,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.07289308545669157 \n",
      " Accuracy: 0.7982423549585596\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00034,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.0728876121268824 \n",
      " Accuracy: 0.7982852243498142\n",
      "────────────────────────────────────────\n",
      "lambda : 0.000285,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.07288247141532281 \n",
      " Accuracy: 0.7983423835381538\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00022999999999999995,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.07287769332659016 \n",
      " Accuracy: 0.7982566447556445\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00017500000000000003,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.07287331601616183 \n",
      " Accuracy: 0.7984138325235781\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00011999999999999999,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.07286938771141108 \n",
      " Accuracy: 0.798313803943984\n",
      "────────────────────────────────────────\n",
      "lambda : 6.499999999999995e-05,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.07286593909214778 \n",
      " Accuracy: 0.7983423835381538\n",
      "────────────────────────────────────────\n",
      "lambda : 1e-05,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.072862082076551 \n",
      " Accuracy: 0.79818519577022\n",
      "════════════════════════════════════════\n",
      "*The optimal hyperparameters*:\n",
      " Model: ridge_regression \n",
      " Optimal accuracy: 0.7985138611031724 \n",
      " Optimal lambda_: 0.0008900000000000001 \n",
      " Optimal gamma: 0.01 \n",
      " \n",
      "┌──────────────────────────────────────┐\n",
      "│               Group: 3               │\n",
      "└──────────────────────────────────────┘\n",
      "────────────────────────────────────────\n",
      "lambda : 0.001,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.031172401315353156 \n",
      " Accuracy: 0.9165343915343916\n",
      "────────────────────────────────────────\n",
      "lambda : 0.000945,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.031166103238089965 \n",
      " Accuracy: 0.9165343915343916\n",
      "────────────────────────────────────────\n",
      "lambda : 0.0008900000000000001,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.031159611027356725 \n",
      " Accuracy: 0.9165343915343916\n",
      "────────────────────────────────────────\n",
      "lambda : 0.000835,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.03115290819904374 \n",
      " Accuracy: 0.9166666666666667\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00078,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.031145977889737615 \n",
      " Accuracy: 0.9165343915343916\n",
      "────────────────────────────────────────\n",
      "lambda : 0.000725,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.031138796661088836 \n",
      " Accuracy: 0.9165343915343916\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00067,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.031131346182708824 \n",
      " Accuracy: 0.9165343915343916\n",
      "────────────────────────────────────────\n",
      "lambda : 0.000615,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.03112358726729369 \n",
      " Accuracy: 0.9165343915343916\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00056,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.03111548990700789 \n",
      " Accuracy: 0.9165343915343916\n",
      "────────────────────────────────────────\n",
      "lambda : 0.000505,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.031107004499336288 \n",
      " Accuracy: 0.9164021164021164\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00045,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.031098067486832075 \n",
      " Accuracy: 0.9165343915343916\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00039499999999999995,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.03108860412010041 \n",
      " Accuracy: 0.9166666666666667\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00034,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.031078509517069735 \n",
      " Accuracy: 0.9166666666666667\n",
      "────────────────────────────────────────\n",
      "lambda : 0.000285,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.031067637538446817 \n",
      " Accuracy: 0.9165343915343916\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00022999999999999995,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.031055770373439366 \n",
      " Accuracy: 0.9165343915343916\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00017500000000000003,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.03104254444968153 \n",
      " Accuracy: 0.9165343915343916\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00011999999999999999,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.03102727834715686 \n",
      " Accuracy: 0.9164021164021164\n",
      "────────────────────────────────────────\n",
      "lambda : 6.499999999999995e-05,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.03101480627839422 \n",
      " Accuracy: 0.9157407407407409\n",
      "────────────────────────────────────────\n",
      "lambda : 1e-05,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.03120352029579156 \n",
      " Accuracy: 0.9154761904761907\n",
      "════════════════════════════════════════\n",
      "*The optimal hyperparameters*:\n",
      " Model: ridge_regression \n",
      " Optimal accuracy: 0.9166666666666667 \n",
      " Optimal lambda_: 0.000835 \n",
      " Optimal gamma: 0.01 \n",
      " \n",
      "┌──────────────────────────────────────┐\n",
      "│               Group: 4               │\n",
      "└──────────────────────────────────────┘\n",
      "────────────────────────────────────────\n",
      "lambda : 0.001,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.06327645081834946 \n",
      " Accuracy: 0.8307297019527236\n",
      "────────────────────────────────────────\n",
      "lambda : 0.000945,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.06326797231158048 \n",
      " Accuracy: 0.8307297019527236\n",
      "────────────────────────────────────────\n",
      "lambda : 0.0008900000000000001,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.06325959287860065 \n",
      " Accuracy: 0.8306856555571869\n",
      "────────────────────────────────────────\n",
      "lambda : 0.000835,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.06325131387565021 \n",
      " Accuracy: 0.8307297019527236\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00078,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.0632431362861188 \n",
      " Accuracy: 0.8308471590074881\n",
      "────────────────────────────────────────\n",
      "lambda : 0.000725,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.06323506065993674 \n",
      " Accuracy: 0.8306416091616503\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00067,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.06322708705974062 \n",
      " Accuracy: 0.8306122448979592\n",
      "────────────────────────────────────────\n",
      "lambda : 0.000615,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.06321921505684339 \n",
      " Accuracy: 0.8306709734253414\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00056,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.0632114437687327 \n",
      " Accuracy: 0.8306856555571869\n",
      "────────────────────────────────────────\n",
      "lambda : 0.000505,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.06320377202142632 \n",
      " Accuracy: 0.8308471590074878\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00045,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.06319619872049179 \n",
      " Accuracy: 0.8308324768756423\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00039499999999999995,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.06318872360102766 \n",
      " Accuracy: 0.8306709734253415\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00034,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.0631813485762646 \n",
      " Accuracy: 0.8305828806342681\n",
      "────────────────────────────────────────\n",
      "lambda : 0.000285,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.0631740800367449 \n",
      " Accuracy: 0.8306416091616503\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00022999999999999995,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.06316693195061887 \n",
      " Accuracy: 0.8306562912934957\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00017500000000000003,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.06315992657963018 \n",
      " Accuracy: 0.8305828806342681\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00011999999999999999,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.06315306921239346 \n",
      " Accuracy: 0.8305681985024226\n",
      "────────────────────────────────────────\n",
      "lambda : 6.499999999999995e-05,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.06314610934436218 \n",
      " Accuracy: 0.8305388342387314\n",
      "────────────────────────────────────────\n",
      "lambda : 1e-05,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.06313555686025726 \n",
      " Accuracy: 0.8307884304801056\n",
      "════════════════════════════════════════\n",
      "*The optimal hyperparameters*:\n",
      " Model: ridge_regression \n",
      " Optimal accuracy: 0.8308471590074881 \n",
      " Optimal lambda_: 0.00078 \n",
      " Optimal gamma: 0.01 \n",
      " \n",
      "┌──────────────────────────────────────┐\n",
      "│               Group: 5               │\n",
      "└──────────────────────────────────────┘\n",
      "────────────────────────────────────────\n",
      "lambda : 0.001,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.03168120874280915 \n",
      " Accuracy: 0.9104072398190046\n",
      "────────────────────────────────────────\n",
      "lambda : 0.000945,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.03167124782652567 \n",
      " Accuracy: 0.9101809954751132\n",
      "────────────────────────────────────────\n",
      "lambda : 0.0008900000000000001,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.03166103627097398 \n",
      " Accuracy: 0.9104072398190046\n",
      "────────────────────────────────────────\n",
      "lambda : 0.000835,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.03165055953064049 \n",
      " Accuracy: 0.9104072398190046\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00078,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.03163980141723864 \n",
      " Accuracy: 0.9104072398190046\n",
      "────────────────────────────────────────\n",
      "lambda : 0.000725,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.03162874374698656 \n",
      " Accuracy: 0.9104072398190046\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00067,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.031617365794668936 \n",
      " Accuracy: 0.9104072398190046\n",
      "────────────────────────────────────────\n",
      "lambda : 0.000615,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.03160564355409501 \n",
      " Accuracy: 0.910633484162896\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00056,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.03159354858039346 \n",
      " Accuracy: 0.9104072398190046\n",
      "────────────────────────────────────────\n",
      "lambda : 0.000505,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.03158104609983632 \n",
      " Accuracy: 0.9104072398190046\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00045,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.03156809183518275 \n",
      " Accuracy: 0.9104072398190046\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00039499999999999995,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.03155462615048159 \n",
      " Accuracy: 0.9104072398190046\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00034,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.031540562713605856 \n",
      " Accuracy: 0.9101809954751132\n",
      "────────────────────────────────────────\n",
      "lambda : 0.000285,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.03152576464852932 \n",
      " Accuracy: 0.9099547511312218\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00022999999999999995,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.031509989330532506 \n",
      " Accuracy: 0.9099547511312218\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00017500000000000003,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.03149274422748341 \n",
      " Accuracy: 0.909502262443439\n",
      "────────────────────────────────────────\n",
      "lambda : 0.00011999999999999999,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.03147284066634579 \n",
      " Accuracy: 0.909502262443439\n",
      "────────────────────────────────────────\n",
      "lambda : 6.499999999999995e-05,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.03144657686710193 \n",
      " Accuracy: 0.9090497737556562\n",
      "────────────────────────────────────────\n",
      "lambda : 1e-05,  gamma: 0.01 \n",
      " \n",
      " Model ridge_regression \n",
      " Loss: 0.031394653274518486 \n",
      " Accuracy: 0.9079185520361992\n",
      "════════════════════════════════════════\n",
      "*The optimal hyperparameters*:\n",
      " Model: ridge_regression \n",
      " Optimal accuracy: 0.910633484162896 \n",
      " Optimal lambda_: 0.000615 \n",
      " Optimal gamma: 0.01 \n",
      " \n",
      "\n",
      " * Overall Accuracy = 0.83220\n"
     ]
    }
   ],
   "source": [
    "data_pro  = data_preprocess_train(tX_train,y_train,outlier_method = 'IQR',n_out = 5)\n",
    "#lambda_ = [1e-2,1e-3,1e-4,1e-5]\n",
    "lambda_ = [1e-4]\n",
    "#lambda_ = np.linspace(1e-3,1e-5,4)\n",
    "gamma_ = [1e-2]\n",
    "accuracy_ = []\n",
    "group_size_ = []\n",
    "optimal_lambdas = []\n",
    "for idx_data,data in enumerate(data_pro):\n",
    "    print('┌'+'─' * 38+'┐')\n",
    "    print('│               Group: %i               │'%idx_data)\n",
    "    print('└'+'─' * 38+'┘')\n",
    "    tx = data[0]\n",
    "    tx = np.hstack((np.ones((tx.shape[0],1)),tx,tx**2,tx**3,tx**4,tx**5, tx**6,tx**7,tx**8,tx**9,tx**11,1/(abs(tx)+1e-10),np.sin(tx)))\n",
    "    y = data[1]\n",
    "    initial_w_ = np.ones(tx.shape[1])\n",
    "    optimal_accuracy, optimal_lambda_, optimal_gamma = Hyperparameter_optimization( y, tx,k_fold=10, \n",
    "                                                                                   lambdas=lambda_ , \n",
    "                                                                                   gammas = gamma_, \n",
    "                                                                                   initial_w = initial_w_, \n",
    "                                                                                   max_iters = 100, \n",
    "                                                                                   model = 'ridge_regression')\n",
    "                                                                                   #model = 'least_squares')\n",
    "    accuracy_.append(optimal_accuracy)\n",
    "    group_size_.append(len(data[2]))\n",
    "    optimal_lambdas.append(optimal_lambda_)\n",
    "\n",
    "accuracy_all = np.dot(accuracy_,group_size_)/np.sum(group_size_)\n",
    "print('\\n * Overall Accuracy = %.5f'%accuracy_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6.499999999999995e-05,\n",
       " 1e-05,\n",
       " 0.0008900000000000001,\n",
       " 0.000835,\n",
       " 0.00078,\n",
       " 0.000615]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_lambdas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model and do prediction on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌──────────────────────────────────────┐\n",
      "│               Group: 0               │\n",
      "└──────────────────────────────────────┘\n",
      "┌──────────────────────────────────────┐\n",
      "│               Group: 1               │\n",
      "└──────────────────────────────────────┘\n",
      "┌──────────────────────────────────────┐\n",
      "│               Group: 2               │\n",
      "└──────────────────────────────────────┘\n",
      "┌──────────────────────────────────────┐\n",
      "│               Group: 3               │\n",
      "└──────────────────────────────────────┘\n",
      "┌──────────────────────────────────────┐\n",
      "│               Group: 4               │\n",
      "└──────────────────────────────────────┘\n",
      "┌──────────────────────────────────────┐\n",
      "│               Group: 5               │\n",
      "└──────────────────────────────────────┘\n",
      "Training finished, start do prediction and write to prediction.csv\n"
     ]
    }
   ],
   "source": [
    "from preprocess_data import *\n",
    "\n",
    "data_pro  = data_preprocess_train(tX_train,y_train,outlier_method = 'IQR',n_out = 5)\n",
    "\n",
    "# Train\n",
    "lambda_ = [6.5e-05,\n",
    " 1e-05,\n",
    " 0.0009,\n",
    " 0.000835,\n",
    " 0.00078,\n",
    " 0.000615]\n",
    "loss_ = [] \n",
    "w_ = []\n",
    "for idx_data,data in enumerate(data_pro):\n",
    "    print('┌'+'─' * 38+'┐')\n",
    "    print('│               Group: %i               │'%idx_data)\n",
    "    print('└'+'─' * 38+'┘')\n",
    "    tx = data[0]\n",
    "    tx = np.hstack((np.ones((tx.shape[0],1)), tx,tx**2,tx**3,tx**4,tx**5,tx**6,tx**8,tx**9,tx**11,1/(abs(tx)+1e-10), np.sin(tx)))\n",
    "    y = data[1]\n",
    "    initial_w_ = np.ones(tx.shape[1])\n",
    "    loss_temp, w_temp = ridge_regression(y, tx,lambda_[idx_data])\n",
    "    w_.append(w_temp)\n",
    "#print('\\n * Overall Accuracy = %.4f'%accuracy_all)\n",
    "\n",
    "## Do predicition on test set\n",
    "data_test  = data_preprocess_predict(tX_test,outlier_method = 'IQR',n_out = 5)\n",
    "print('Training finished, start do prediction and write to prediction.csv')\n",
    "# Using trained model to do prediction\n",
    "label_ = []\n",
    "data_size = sum([len(data_test[:,1][i]) for i in range(6)])\n",
    "y_predict = np.zeros(data_size)\n",
    "for idx_data,data in enumerate(data_test):\n",
    "    tx = data[0]\n",
    "    tx = np.hstack((np.ones((tx.shape[0],1)), tx,tx**2,tx**3,tx**4,tx**5,tx**6,tx**8,tx**9,tx**11,1/(abs(tx)+1e-10), np.sin(tx)))\n",
    "    label_.append(predict_labels(w_[idx_data], tx,model='ridge_regression'))\n",
    "    y_predict[data[1]] = predict_labels(w_[idx_data], tx,model='ridge_regression')\n",
    "\n",
    "create_csv_submission(range(350000,350000+data_size),(y_predict-0.5)*2,'prediction.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌──────────────────────────────────────┐\n",
      "│               Group: 0               │\n",
      "└──────────────────────────────────────┘\n",
      "┌──────────────────────────────────────┐\n",
      "│               Group: 1               │\n",
      "└──────────────────────────────────────┘\n",
      "┌──────────────────────────────────────┐\n",
      "│               Group: 2               │\n",
      "└──────────────────────────────────────┘\n",
      "┌──────────────────────────────────────┐\n",
      "│               Group: 3               │\n",
      "└──────────────────────────────────────┘\n",
      "┌──────────────────────────────────────┐\n",
      "│               Group: 4               │\n",
      "└──────────────────────────────────────┘\n",
      "┌──────────────────────────────────────┐\n",
      "│               Group: 5               │\n",
      "└──────────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "lambda_ = [0]\n",
    "gamma_ = [1e-2]\n",
    "accuracy_ = []\n",
    "group_size_ = []\n",
    "loss_ = [] \n",
    "w_ = []\n",
    "for idx_data,data in enumerate(data_pro):\n",
    "    print('┌'+'─' * 38+'┐')\n",
    "    print('│               Group: %i               │'%idx_data)\n",
    "    print('└'+'─' * 38+'┘')\n",
    "    tx = data[0]\n",
    "    tx = np.hstack((np.ones((tx.shape[0],1)), tx,tx**2,tx**3,tx**4,tx**5,tx**6,tx**8,tx**9, np.sin(tx)))\n",
    "    y = data[1]\n",
    "    initial_w_ = np.ones(tx.shape[1])\n",
    "    loss_temp, w_temp = least_squares(y, tx)\n",
    "    w_.append(w_temp)\n",
    "#print('\\n * Overall Accuracy = %.4f'%accuracy_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8055834123865022, 0.9479385981701949, 0.7956617415906948, 0.918804549061095, 0.8279355198637578, 0.9166854820501242]\n"
     ]
    }
   ],
   "source": [
    "# Using trained model to do prediction\n",
    "acc_ = []\n",
    "for idx_data,data in enumerate(data_pro):\n",
    "    tx = data[0]\n",
    "    tx = np.hstack((np.ones((tx.shape[0],1)), tx,tx**2,tx**3,tx**4,tx**5,tx**6,tx**8,tx**9, np.sin(tx)))\n",
    "    y = data[1]\n",
    "    acc_.append(compute_accuracy(tx, y, w_[idx_data], model='least_squares'))\n",
    "print(acc_)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = np.zeros(sum([len(data_pro[:,2][i]) for i in range(6)]))\n",
    "for idx_data,data in enumerate(data_pro):\n",
    "    tx = data[0]\n",
    "    tx = np.hstack((np.ones((tx.shape[0],1)), tx,tx**2,tx**3,tx**4,tx**5,tx**6,tx**8,tx**9, np.sin(tx)))\n",
    "    #label_.append(predict_labels(w_[idx_data], tx,model='least_squares'))\n",
    "    y_predict[data[2]] = predict_labels(w_[idx_data], tx,model='least_squares')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test set\n",
    "DATA_TRAIN_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "feature_names = load_headers(DATA_TRAIN_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test  = data_preprocess_predict(tX,outlier_method = 'IQR',n_out = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using trained model to do prediction\n",
    "acc_ = []\n",
    "label_ = []\n",
    "data_size = sum([len(data_test[:,1][i]) for i in range(6)])\n",
    "y_predict = np.zeros(data_size)\n",
    "for idx_data,data in enumerate(data_test):\n",
    "    tx = data[0]\n",
    "    tx = np.hstack((np.ones((tx.shape[0],1)), tx,tx**2,tx**3,tx**4,tx**5,tx**6,tx**8,tx**9, np.sin(tx)))\n",
    "    label_.append(predict_labels(w_[idx_data], tx,model='least_squares'))\n",
    "    y_predict[data[1]] = predict_labels(w_[idx_data], tx,model='least_squares')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(range(350000,350000+data_size),(y_predict-0.5)*2,'prediction.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568238, 30)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌──────────────────────────────────────┐\n",
      "│               Group: 0               │\n",
      "└──────────────────────────────────────┘\n",
      "┌──────────────────────────────────────┐\n",
      "│               Group: 1               │\n",
      "└──────────────────────────────────────┘\n",
      "┌──────────────────────────────────────┐\n",
      "│               Group: 2               │\n",
      "└──────────────────────────────────────┘\n",
      "┌──────────────────────────────────────┐\n",
      "│               Group: 3               │\n",
      "└──────────────────────────────────────┘\n",
      "┌──────────────────────────────────────┐\n",
      "│               Group: 4               │\n",
      "└──────────────────────────────────────┘\n",
      "┌──────────────────────────────────────┐\n",
      "│               Group: 5               │\n",
      "└──────────────────────────────────────┘\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Hyperparameter_optimization import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌──────────────────────────────────────┐\n",
      "│               Group: 0               │\n",
      "└──────────────────────────────────────┘\n",
      "────────────────────────────────────────\n",
      "lambda : 0,  gamma: 0.01 \n",
      " \n",
      " Model least_squares \n",
      " Loss: 0.13134087391240726 \n",
      " Accuracy: 0.6929717569252453\n",
      "════════════════════════════════════════\n",
      "*The optimal hyperparameters*:\n",
      " Model: least_squares \n",
      " Optimal accuracy: 0.6929717569252453 \n",
      " Optimal lambda_: 0 \n",
      " Optimal gamma: 0.01 \n",
      " \n",
      "┌──────────────────────────────────────┐\n",
      "│               Group: 1               │\n",
      "└──────────────────────────────────────┘\n",
      "────────────────────────────────────────\n",
      "lambda : 0,  gamma: 0.01 \n",
      " \n",
      " Model least_squares \n",
      " Loss: 0.026579363200754248 \n",
      " Accuracy: 0.9403139356814703\n",
      "════════════════════════════════════════\n",
      "*The optimal hyperparameters*:\n",
      " Model: least_squares \n",
      " Optimal accuracy: 0.9403139356814703 \n",
      " Optimal lambda_: 0 \n",
      " Optimal gamma: 0.01 \n",
      " \n",
      "┌──────────────────────────────────────┐\n",
      "│               Group: 2               │\n",
      "└──────────────────────────────────────┘\n",
      "────────────────────────────────────────\n",
      "lambda : 0,  gamma: 0.01 \n",
      " \n",
      " Model least_squares \n",
      " Loss: 0.1727353058676719 \n",
      " Accuracy: 0.6146470420120035\n",
      "════════════════════════════════════════\n",
      "*The optimal hyperparameters*:\n",
      " Model: least_squares \n",
      " Optimal accuracy: 0.6146470420120035 \n",
      " Optimal lambda_: 0 \n",
      " Optimal gamma: 0.01 \n",
      " \n",
      "┌──────────────────────────────────────┐\n",
      "│               Group: 3               │\n",
      "└──────────────────────────────────────┘\n",
      "────────────────────────────────────────\n",
      "lambda : 0,  gamma: 0.01 \n",
      " \n",
      " Model least_squares \n",
      " Loss: 0.040550807001849036 \n",
      " Accuracy: 0.9079365079365078\n",
      "════════════════════════════════════════\n",
      "*The optimal hyperparameters*:\n",
      " Model: least_squares \n",
      " Optimal accuracy: 0.9079365079365078 \n",
      " Optimal lambda_: 0 \n",
      " Optimal gamma: 0.01 \n",
      " \n",
      "┌──────────────────────────────────────┐\n",
      "│               Group: 4               │\n",
      "└──────────────────────────────────────┘\n",
      "────────────────────────────────────────\n",
      "lambda : 0,  gamma: 0.01 \n",
      " \n",
      " Model least_squares \n",
      " Loss: 0.20316163906089119 \n",
      " Accuracy: 0.5456160441625558\n",
      "════════════════════════════════════════\n",
      "*The optimal hyperparameters*:\n",
      " Model: least_squares \n",
      " Optimal accuracy: 0.5456160441625558 \n",
      " Optimal lambda_: 0 \n",
      " Optimal gamma: 0.01 \n",
      " \n",
      "┌──────────────────────────────────────┐\n",
      "│               Group: 5               │\n",
      "└──────────────────────────────────────┘\n",
      "────────────────────────────────────────\n",
      "lambda : 0,  gamma: 0.01 \n",
      " \n",
      " Model least_squares \n",
      " Loss: 0.04901779472360017 \n",
      " Accuracy: 0.8839205058717254\n",
      "════════════════════════════════════════\n",
      "*The optimal hyperparameters*:\n",
      " Model: least_squares \n",
      " Optimal accuracy: 0.8839205058717254 \n",
      " Optimal lambda_: 0 \n",
      " Optimal gamma: 0.01 \n",
      " \n",
      "\n",
      " * Overall Accuracy = 0.6666\n"
     ]
    }
   ],
   "source": [
    "lambda_ = [0]\n",
    "gamma_ = [1e-2]\n",
    "accuracy_ = []\n",
    "group_size_ = []\n",
    "for idx_data,data in enumerate(data_pro):\n",
    "    print('┌'+'─' * 38+'┐')\n",
    "    print('│               Group: %i               │'%idx_data)\n",
    "    print('└'+'─' * 38+'┘')\n",
    "    tx = data[0]\n",
    "    y = data[1]\n",
    "    initial_w_ = np.ones(tx.shape[1])\n",
    "    optimal_accuracy, optimal_lambda_, optimal_gamma = Hyperparameter_optimization( y, tx,k_fold=4, \n",
    "                                                                                   lambdas=lambda_ , \n",
    "                                                                                   gammas = gamma_, \n",
    "                                                                                   initial_w = initial_w_, \n",
    "                                                                                   max_iters = 100, \n",
    "                                                                                   model = 'least_squares')\n",
    "    accuracy_.append(optimal_accuracy)\n",
    "    group_size_.append(len(data[2]))\n",
    "\n",
    "accuracy_all = np.dot(accuracy_,group_size_)/np.sum(group_size_)\n",
    "print('\\n * Overall Accuracy = %.4f'%accuracy_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With bias and second order "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌──────────────────────────────────────┐\n",
      "│               Group: 0               │\n",
      "└──────────────────────────────────────┘\n",
      "────────────────────────────────────────\n",
      "lambda : 0,  gamma: 0.01 \n",
      " \n",
      " Model least_squares \n",
      " Loss: 0.0728864008015079 \n",
      " Accuracy: 0.7952377080284058\n",
      "════════════════════════════════════════\n",
      "*The optimal hyperparameters*:\n",
      " Model: least_squares \n",
      " Optimal accuracy: 0.7952377080284058 \n",
      " Optimal lambda_: 0 \n",
      " Optimal gamma: 0.01 \n",
      " \n",
      "┌──────────────────────────────────────┐\n",
      "│               Group: 1               │\n",
      "└──────────────────────────────────────┘\n",
      "────────────────────────────────────────\n",
      "lambda : 0,  gamma: 0.01 \n",
      " \n",
      " Model least_squares \n",
      " Loss: 0.02309335642638366 \n",
      " Accuracy: 0.9424961715160797\n",
      "════════════════════════════════════════\n",
      "*The optimal hyperparameters*:\n",
      " Model: least_squares \n",
      " Optimal accuracy: 0.9424961715160797 \n",
      " Optimal lambda_: 0 \n",
      " Optimal gamma: 0.01 \n",
      " \n",
      "┌──────────────────────────────────────┐\n",
      "│               Group: 2               │\n",
      "└──────────────────────────────────────┘\n",
      "────────────────────────────────────────\n",
      "lambda : 0,  gamma: 0.01 \n",
      " \n",
      " Model least_squares \n",
      " Loss: 0.08334291705793892 \n",
      " Accuracy: 0.7686053158045156\n",
      "════════════════════════════════════════\n",
      "*The optimal hyperparameters*:\n",
      " Model: least_squares \n",
      " Optimal accuracy: 0.7686053158045156 \n",
      " Optimal lambda_: 0 \n",
      " Optimal gamma: 0.01 \n",
      " \n",
      "┌──────────────────────────────────────┐\n",
      "│               Group: 3               │\n",
      "└──────────────────────────────────────┘\n",
      "────────────────────────────────────────\n",
      "lambda : 0,  gamma: 0.01 \n",
      " \n",
      " Model least_squares \n",
      " Loss: 0.0347691629226481 \n",
      " Accuracy: 0.9121693121693122\n",
      "════════════════════════════════════════\n",
      "*The optimal hyperparameters*:\n",
      " Model: least_squares \n",
      " Optimal accuracy: 0.9121693121693122 \n",
      " Optimal lambda_: 0 \n",
      " Optimal gamma: 0.01 \n",
      " \n",
      "┌──────────────────────────────────────┐\n",
      "│               Group: 4               │\n",
      "└──────────────────────────────────────┘\n",
      "────────────────────────────────────────\n",
      "lambda : 0,  gamma: 0.01 \n",
      " \n",
      " Model least_squares \n",
      " Loss: 0.07603386377311896 \n",
      " Accuracy: 0.8012978623443741\n",
      "════════════════════════════════════════\n",
      "*The optimal hyperparameters*:\n",
      " Model: least_squares \n",
      " Optimal accuracy: 0.8012978623443741 \n",
      " Optimal lambda_: 0 \n",
      " Optimal gamma: 0.01 \n",
      " \n",
      "┌──────────────────────────────────────┐\n",
      "│               Group: 5               │\n",
      "└──────────────────────────────────────┘\n",
      "────────────────────────────────────────\n",
      "lambda : 0,  gamma: 0.01 \n",
      " \n",
      " Model least_squares \n",
      " Loss: 0.036991840374433635 \n",
      " Accuracy: 0.9035682023486902\n",
      "════════════════════════════════════════\n",
      "*The optimal hyperparameters*:\n",
      " Model: least_squares \n",
      " Optimal accuracy: 0.9035682023486902 \n",
      " Optimal lambda_: 0 \n",
      " Optimal gamma: 0.01 \n",
      " \n",
      "\n",
      " * Overall Accuracy = 0.8103\n"
     ]
    }
   ],
   "source": [
    "lambda_ = [0]\n",
    "gamma_ = [1e-2]\n",
    "accuracy_ = []\n",
    "group_size_ = []\n",
    "for idx_data,data in enumerate(data_pro):\n",
    "    print('┌'+'─' * 38+'┐')\n",
    "    print('│               Group: %i               │'%idx_data)\n",
    "    print('└'+'─' * 38+'┘')\n",
    "    tx = data[0]\n",
    "    tx = np.hstack((np.ones((tx.shape[0],1)), tx,tx**2))\n",
    "    y = data[1]\n",
    "    initial_w_ = np.ones(tx.shape[1])\n",
    "    optimal_accuracy, optimal_lambda_, optimal_gamma = Hyperparameter_optimization( y, tx,k_fold=4, \n",
    "                                                                                   lambdas=lambda_ , \n",
    "                                                                                   gammas = gamma_, \n",
    "                                                                                   initial_w = initial_w_, \n",
    "                                                                                   max_iters = 100, \n",
    "                                                                                   model = 'least_squares')\n",
    "    accuracy_.append(optimal_accuracy)\n",
    "    group_size_.append(len(data[2]))\n",
    "\n",
    "accuracy_all = np.dot(accuracy_,group_size_)/np.sum(group_size_)\n",
    "print('\\n * Overall Accuracy = %.4f'%accuracy_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try higher orders and non-linear term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌──────────────────────────────────────┐\n",
      "│               Group: 0               │\n",
      "└──────────────────────────────────────┘\n",
      "────────────────────────────────────────\n",
      "lambda : 0,  gamma: 1e-05 \n",
      " \n",
      "Current iteration=0, loss=nan\n",
      "Current iteration=100, loss=31476.345236940593\n",
      "Current iteration=200, loss=31417.107922883464\n",
      "Current iteration=300, loss=31405.56055855738\n",
      "Current iteration=400, loss=31403.046397704355\n",
      "Current iteration=500, loss=31402.487016534564\n",
      "Current iteration=600, loss=31402.361488007817\n",
      "Current iteration=700, loss=31402.333181678572\n",
      "Current iteration=800, loss=31402.32677903084\n",
      "Current iteration=900, loss=31402.32532780975\n",
      "Current iteration=0, loss=31454.457524724592\n",
      "Current iteration=100, loss=31452.63855977228\n",
      "Current iteration=200, loss=31452.488950897867\n",
      "Current iteration=300, loss=31452.46009258678\n",
      "Current iteration=400, loss=31452.454141875187\n",
      "Current iteration=500, loss=31452.452824808177\n",
      "Current iteration=600, loss=31452.4525124258\n",
      "Current iteration=700, loss=31452.452433780523\n",
      "Current iteration=800, loss=31452.452413049665\n",
      "Current iteration=900, loss=31452.452407405086\n",
      "Current iteration=0, loss=31412.792437587086\n",
      "Current iteration=100, loss=31411.891485694687\n",
      "Current iteration=200, loss=31411.854687207975\n",
      "Current iteration=300, loss=31411.845417450335\n",
      "Current iteration=400, loss=31411.842935945966\n",
      "Current iteration=500, loss=31411.842254197465\n",
      "Current iteration=600, loss=31411.842063756987\n",
      "Current iteration=700, loss=31411.842009955733\n",
      "Current iteration=800, loss=31411.841994638507\n",
      "Current iteration=900, loss=31411.841990254245\n",
      "Current iteration=0, loss=31447.96222587844\n",
      "Current iteration=100, loss=31446.364786539933\n",
      "Current iteration=200, loss=31446.307531667866\n",
      "Current iteration=300, loss=31446.296516638646\n",
      "Current iteration=400, loss=31446.294268772544\n",
      "Current iteration=500, loss=31446.293785243113\n",
      "Current iteration=600, loss=31446.293675627152\n",
      "Current iteration=700, loss=31446.293649515275\n",
      "Current iteration=800, loss=31446.293643016284\n",
      "Current iteration=900, loss=31446.293641338012\n",
      "Current iteration=0, loss=31444.553008027113\n",
      "Current iteration=100, loss=31442.432965337503\n",
      "Current iteration=200, loss=31442.381599071254\n",
      "Current iteration=300, loss=31442.371679571188\n",
      "Current iteration=400, loss=31442.36937543714\n",
      "Current iteration=500, loss=31442.368817970953\n",
      "Current iteration=600, loss=31442.36867963873\n",
      "Current iteration=700, loss=31442.36864453985\n",
      "Current iteration=800, loss=31442.36863545132\n",
      "Current iteration=900, loss=31442.368633053968\n",
      "Current iteration=0, loss=31351.611411737256\n",
      "Current iteration=100, loss=31348.371195675234\n",
      "Current iteration=200, loss=31348.138574726043\n",
      "Current iteration=300, loss=31348.074983950166\n",
      "Current iteration=400, loss=31348.056030794585\n",
      "Current iteration=500, loss=31348.050338717792\n",
      "Current iteration=600, loss=31348.048626563257\n",
      "Current iteration=700, loss=31348.048111031017\n",
      "Current iteration=800, loss=31348.04795567516\n",
      "Current iteration=900, loss=31348.047908827237\n",
      "Current iteration=0, loss=31456.74050554728\n",
      "Current iteration=100, loss=31454.098282231702\n",
      "Current iteration=200, loss=31453.905913325165\n",
      "Current iteration=300, loss=31453.860844694107\n",
      "Current iteration=400, loss=31453.849304990126\n",
      "Current iteration=500, loss=31453.846259941904\n",
      "Current iteration=600, loss=31453.84543969022\n",
      "Current iteration=700, loss=31453.845215124922\n",
      "Current iteration=800, loss=31453.84515284199\n",
      "Current iteration=900, loss=31453.845135387426\n",
      "Current iteration=0, loss=31383.13382433754\n",
      "Current iteration=100, loss=31381.72508738064\n",
      "Current iteration=200, loss=31381.650917889754\n",
      "Current iteration=300, loss=31381.637548667342\n",
      "Current iteration=400, loss=31381.63504008409\n",
      "Current iteration=500, loss=31381.634557902817\n",
      "Current iteration=600, loss=31381.634462775815\n",
      "Current iteration=700, loss=31381.634443461004\n",
      "Current iteration=800, loss=31381.634439416484\n",
      "Current iteration=900, loss=31381.634438542063\n",
      "Current iteration=0, loss=31408.250297880357\n",
      "Current iteration=100, loss=31406.885218986703\n",
      "Current iteration=200, loss=31406.828212757708\n",
      "Current iteration=300, loss=31406.813199848373\n",
      "Current iteration=400, loss=31406.80908081888\n",
      "Current iteration=500, loss=31406.807925732595\n",
      "Current iteration=600, loss=31406.80759740688\n",
      "Current iteration=700, loss=31406.8075033107\n",
      "Current iteration=800, loss=31406.8074762099\n",
      "Current iteration=900, loss=31406.807468381674\n",
      "Current iteration=0, loss=31464.797968926076\n",
      "Current iteration=100, loss=31463.035483193118\n",
      "Current iteration=200, loss=31462.93226276808\n",
      "Current iteration=300, loss=31462.907387013758\n",
      "Current iteration=400, loss=31462.900877323274\n",
      "Current iteration=500, loss=31462.89908715293\n",
      "Current iteration=600, loss=31462.89857867916\n",
      "Current iteration=700, loss=31462.898431310357\n",
      "Current iteration=800, loss=31462.89838807831\n",
      "Current iteration=900, loss=31462.898375305253\n",
      " Model logistic_regression \n",
      " Loss: 31420.85149692485 \n",
      " Accuracy: 0.7785201246781407\n",
      "════════════════════════════════════════\n",
      "*The optimal hyperparameters*:\n",
      " Model: logistic_regression \n",
      " Optimal accuracy: 0.7785201246781407 \n",
      " Optimal lambda_: 0 \n",
      " Optimal gamma: 1e-05 \n",
      " \n",
      "┌──────────────────────────────────────┐\n",
      "│               Group: 1               │\n",
      "└──────────────────────────────────────┘\n",
      "────────────────────────────────────────\n",
      "lambda : 0,  gamma: 1e-05 \n",
      " \n",
      "Current iteration=0, loss=nan\n",
      "Current iteration=100, loss=5276.6926059491725\n",
      "Current iteration=200, loss=4541.214682706017\n",
      "Current iteration=300, loss=4373.775464526567\n",
      "Current iteration=400, loss=4314.642508479867\n",
      "Current iteration=500, loss=4291.783172503434\n",
      "Current iteration=600, loss=4282.431617296021\n",
      "Current iteration=700, loss=4278.3014500245645\n",
      "Current iteration=800, loss=4276.294972045615\n",
      "Current iteration=900, loss=4275.21603651402\n",
      "Current iteration=0, loss=4295.09952643518\n",
      "Current iteration=100, loss=4292.926707170809\n",
      "Current iteration=200, loss=4292.219809975049\n",
      "Current iteration=300, loss=4291.8964341281335\n",
      "Current iteration=400, loss=4291.7346757349205\n",
      "Current iteration=500, loss=4291.647520729182\n",
      "Current iteration=600, loss=4291.596964973978\n",
      "Current iteration=700, loss=4291.565498523519\n",
      "Current iteration=800, loss=4291.544675694644\n",
      "Current iteration=900, loss=4291.530213737199\n",
      "Current iteration=0, loss=4287.365944277525\n",
      "Current iteration=100, loss=4285.764114023981\n",
      "Current iteration=200, loss=4285.487145287076\n",
      "Current iteration=300, loss=4285.3982102056025\n",
      "Current iteration=400, loss=4285.360789876122\n",
      "Current iteration=500, loss=4285.341943405049\n",
      "Current iteration=600, loss=4285.3311436068725\n",
      "Current iteration=700, loss=4285.324328888319\n",
      "Current iteration=800, loss=4285.319714796793\n",
      "Current iteration=900, loss=4285.316434336292\n",
      "Current iteration=0, loss=4313.015438446015\n",
      "Current iteration=100, loss=4311.985143599886\n",
      "Current iteration=200, loss=4311.8743160361355\n",
      "Current iteration=300, loss=4311.836952402471\n",
      "Current iteration=400, loss=4311.820551636361\n",
      "Current iteration=500, loss=4311.81254426454\n",
      "Current iteration=600, loss=4311.808339871356\n",
      "Current iteration=700, loss=4311.805991136624\n",
      "Current iteration=800, loss=4311.804601818792\n",
      "Current iteration=900, loss=4311.803735510707\n",
      "Current iteration=0, loss=4291.638236691408\n",
      "Current iteration=100, loss=4289.9125920077195\n",
      "Current iteration=200, loss=4289.730853382066\n",
      "Current iteration=300, loss=4289.694880482573\n",
      "Current iteration=400, loss=4289.684523488082\n",
      "Current iteration=500, loss=4289.680360022257\n",
      "Current iteration=600, loss=4289.678270435386\n",
      "Current iteration=700, loss=4289.677091628416\n",
      "Current iteration=800, loss=4289.676383692522\n",
      "Current iteration=900, loss=4289.675940887887\n",
      "Current iteration=0, loss=4333.903290435627\n",
      "Current iteration=100, loss=4333.032520746685\n",
      "Current iteration=200, loss=4332.854856663459\n",
      "Current iteration=300, loss=4332.782785196592\n",
      "Current iteration=400, loss=4332.745173804248\n",
      "Current iteration=500, loss=4332.722968183565\n",
      "Current iteration=600, loss=4332.70904130879\n",
      "Current iteration=700, loss=4332.699976772221\n",
      "Current iteration=800, loss=4332.6939060190325\n",
      "Current iteration=900, loss=4332.689739942614\n",
      "Current iteration=0, loss=4326.533493456147\n",
      "Current iteration=100, loss=4325.121208838732\n",
      "Current iteration=200, loss=4324.863657980122\n",
      "Current iteration=300, loss=4324.769596549151\n",
      "Current iteration=400, loss=4324.724482686748\n",
      "Current iteration=500, loss=4324.7002605964035\n",
      "Current iteration=600, loss=4324.686612782887\n",
      "Current iteration=700, loss=4324.6787258345485\n",
      "Current iteration=800, loss=4324.674082716821\n",
      "Current iteration=900, loss=4324.671300589083\n",
      "Current iteration=0, loss=4267.8654191861915\n",
      "Current iteration=100, loss=4265.781992597436\n",
      "Current iteration=200, loss=4265.50574553015\n",
      "Current iteration=300, loss=4265.413452141166\n",
      "Current iteration=400, loss=4265.372655360019\n",
      "Current iteration=500, loss=4265.351495277713\n",
      "Current iteration=600, loss=4265.3392735090765\n",
      "Current iteration=700, loss=4265.33160949791\n",
      "Current iteration=800, loss=4265.32648920933\n",
      "Current iteration=900, loss=4265.322904826276\n",
      "Current iteration=0, loss=4294.50014296846\n",
      "Current iteration=100, loss=4293.223232580969\n",
      "Current iteration=200, loss=4293.04117662653\n",
      "Current iteration=300, loss=4292.993960492722\n",
      "Current iteration=400, loss=4292.9751407784615\n",
      "Current iteration=500, loss=4292.964994803382\n",
      "Current iteration=600, loss=4292.958519737641\n",
      "Current iteration=700, loss=4292.954012134379\n",
      "Current iteration=800, loss=4292.95073160373\n",
      "Current iteration=900, loss=4292.948288420173\n",
      "Current iteration=0, loss=4293.947304986957\n",
      "Current iteration=100, loss=4292.958038165696\n",
      "Current iteration=200, loss=4292.6669145961005\n",
      "Current iteration=300, loss=4292.5360471979475\n",
      "Current iteration=400, loss=4292.464815749158\n",
      "Current iteration=500, loss=4292.421463654666\n",
      "Current iteration=600, loss=4292.393155260672\n",
      "Current iteration=700, loss=4292.373768164318\n",
      "Current iteration=800, loss=4292.360040603458\n",
      "Current iteration=900, loss=4292.350088665751\n",
      " Model logistic_regression \n",
      " Loss: 4296.086306771245 \n",
      " Accuracy: 0.9405053598774886\n",
      "════════════════════════════════════════\n",
      "*The optimal hyperparameters*:\n",
      " Model: logistic_regression \n",
      " Optimal accuracy: 0.9405053598774886 \n",
      " Optimal lambda_: 0 \n",
      " Optimal gamma: 1e-05 \n",
      " \n",
      "┌──────────────────────────────────────┐\n",
      "│               Group: 2               │\n",
      "└──────────────────────────────────────┘\n",
      "────────────────────────────────────────\n",
      "lambda : 0,  gamma: 1e-05 \n",
      " \n",
      "Current iteration=0, loss=nan\n",
      "Current iteration=100, loss=51673.98437462776\n",
      "Current iteration=200, loss=51445.24513394928\n",
      "Current iteration=300, loss=51415.90691416301\n",
      "Current iteration=400, loss=51405.76954301236\n",
      "Current iteration=500, loss=51401.283502733524\n",
      "Current iteration=600, loss=51399.22290421916\n",
      "Current iteration=700, loss=51398.275455453324\n",
      "Current iteration=800, loss=51397.842130641526\n",
      "Current iteration=900, loss=51397.645007209314\n",
      "Current iteration=0, loss=51485.44566370019\n",
      "Current iteration=100, loss=51456.99504407042\n",
      "Current iteration=200, loss=51460.59578594486\n",
      "Current iteration=300, loss=51461.98445994681\n",
      "Current iteration=400, loss=51462.6221829324\n",
      "Current iteration=500, loss=51462.903600944985\n",
      "Current iteration=600, loss=51463.0260294194\n",
      "Current iteration=700, loss=51463.079467246236\n",
      "Current iteration=800, loss=51463.103009353865\n",
      "Current iteration=900, loss=51463.11348448273\n",
      "Current iteration=0, loss=51587.762395849626\n",
      "Current iteration=100, loss=51610.411049473274\n",
      "Current iteration=200, loss=51604.5503997519\n",
      "Current iteration=300, loss=51602.23301706021\n",
      "Current iteration=400, loss=51601.28923403946\n",
      "Current iteration=500, loss=51600.90426372366\n",
      "Current iteration=600, loss=51600.745413776545\n",
      "Current iteration=700, loss=51600.67871855022\n",
      "Current iteration=800, loss=51600.65019579723\n",
      "Current iteration=900, loss=51600.63779151908\n",
      "Current iteration=0, loss=51296.8458123422\n",
      "Current iteration=100, loss=51216.330505345715\n",
      "Current iteration=200, loss=51219.43844704756\n",
      "Current iteration=300, loss=51220.63747390284\n",
      "Current iteration=400, loss=51220.95786836238\n",
      "Current iteration=500, loss=51221.037549190325\n",
      "Current iteration=600, loss=51221.05504301936\n",
      "Current iteration=700, loss=51221.05754392651\n",
      "Current iteration=800, loss=51221.05708015283\n",
      "Current iteration=900, loss=51221.0564071215\n",
      "Current iteration=0, loss=51332.49221770807\n",
      "Current iteration=100, loss=51411.602547851966\n",
      "Current iteration=200, loss=51411.40442260265\n",
      "Current iteration=300, loss=51411.19477056599\n",
      "Current iteration=400, loss=51411.20599118814\n",
      "Current iteration=500, loss=51411.24483803354\n",
      "Current iteration=600, loss=51411.272769762116\n",
      "Current iteration=700, loss=51411.28884095809\n",
      "Current iteration=800, loss=51411.29731673736\n",
      "Current iteration=900, loss=51411.30158598003\n",
      "Current iteration=0, loss=51588.20886175519\n",
      "Current iteration=100, loss=51447.65432389226\n",
      "Current iteration=200, loss=51441.75872238963\n",
      "Current iteration=300, loss=51439.94854118358\n",
      "Current iteration=400, loss=51439.253559605786\n",
      "Current iteration=500, loss=51438.96121936709\n",
      "Current iteration=600, loss=51438.83277250129\n",
      "Current iteration=700, loss=51438.775061217326\n",
      "Current iteration=800, loss=51438.748815070845\n",
      "Current iteration=900, loss=51438.736795742254\n",
      "Current iteration=0, loss=51203.38406480319\n",
      "Current iteration=100, loss=51253.06078095663\n",
      "Current iteration=200, loss=51256.10740546327\n",
      "Current iteration=300, loss=51257.007431670725\n",
      "Current iteration=400, loss=51257.34421030792\n",
      "Current iteration=500, loss=51257.47698165154\n",
      "Current iteration=600, loss=51257.529941759734\n",
      "Current iteration=700, loss=51257.55112456359\n",
      "Current iteration=800, loss=51257.559599649576\n",
      "Current iteration=900, loss=51257.562988846614\n",
      "Current iteration=0, loss=51163.918382173026\n",
      "Current iteration=100, loss=51220.284904393484\n",
      "Current iteration=200, loss=51220.80663780681\n",
      "Current iteration=300, loss=51220.96803470544\n",
      "Current iteration=400, loss=51220.97703047143\n",
      "Current iteration=500, loss=51220.96489022367\n",
      "Current iteration=600, loss=51220.955417199984\n",
      "Current iteration=700, loss=51220.95004538241\n",
      "Current iteration=800, loss=51220.947284000176\n",
      "Current iteration=900, loss=51220.945926321845\n",
      "Current iteration=0, loss=51408.74290058262\n",
      "Current iteration=100, loss=51382.40818842473\n",
      "Current iteration=200, loss=51378.2297133888\n",
      "Current iteration=300, loss=51376.8469296178\n",
      "Current iteration=400, loss=51376.33503763888\n",
      "Current iteration=500, loss=51376.133538017515\n",
      "Current iteration=600, loss=51376.05155597068\n",
      "Current iteration=700, loss=51376.01759773253\n",
      "Current iteration=800, loss=51376.003376859895\n",
      "Current iteration=900, loss=51375.9973737276\n",
      "Current iteration=0, loss=51280.30251729669\n",
      "Current iteration=100, loss=51214.571868031824\n",
      "Current iteration=200, loss=51217.50285996614\n",
      "Current iteration=300, loss=51218.753153753285\n",
      "Current iteration=400, loss=51219.28950151513\n",
      "Current iteration=500, loss=51219.52129328497\n",
      "Current iteration=600, loss=51219.62213657312\n",
      "Current iteration=700, loss=51219.666219725535\n",
      "Current iteration=800, loss=51219.68556278802\n",
      "Current iteration=900, loss=51219.69407805552\n",
      " Model logistic_regression \n",
      " Loss: 55603.39153268407 \n",
      " Accuracy: 0.626750500142898\n",
      "════════════════════════════════════════\n",
      "*The optimal hyperparameters*:\n",
      " Model: logistic_regression \n",
      " Optimal accuracy: 0.626750500142898 \n",
      " Optimal lambda_: 0 \n",
      " Optimal gamma: 1e-05 \n",
      " \n",
      "┌──────────────────────────────────────┐\n",
      "│               Group: 3               │\n",
      "└──────────────────────────────────────┘\n",
      "────────────────────────────────────────\n",
      "lambda : 0,  gamma: 1e-05 \n",
      " \n",
      "Current iteration=0, loss=nan\n",
      "Current iteration=100, loss=3698.8778361331065\n",
      "Current iteration=200, loss=2551.199855006623\n",
      "Current iteration=300, loss=2138.9942737630026\n",
      "Current iteration=400, loss=1953.0191465534622\n",
      "Current iteration=500, loss=1857.943208350863\n",
      "Current iteration=600, loss=1804.416083528006\n",
      "Current iteration=700, loss=1771.9151642965871\n",
      "Current iteration=800, loss=1751.3114623108736\n",
      "Current iteration=900, loss=1737.9602219933101\n",
      "Current iteration=0, loss=1713.043143761387\n",
      "Current iteration=100, loss=1706.7490680715857\n",
      "Current iteration=200, loss=1702.8443282114874\n",
      "Current iteration=300, loss=1700.262689852159\n",
      "Current iteration=400, loss=1698.5162648506807\n",
      "Current iteration=500, loss=1697.3143048337106\n",
      "Current iteration=600, loss=1696.4748768960526\n",
      "Current iteration=700, loss=1695.8809725037809\n",
      "Current iteration=800, loss=1695.4558257339604\n",
      "Current iteration=900, loss=1695.1482401899996\n",
      "Current iteration=0, loss=1701.9685732160146\n",
      "Current iteration=100, loss=1700.601306250567\n",
      "Current iteration=200, loss=1699.9292406008378\n",
      "Current iteration=300, loss=1699.5164651283776\n",
      "Current iteration=400, loss=1699.2319374829872\n",
      "Current iteration=500, loss=1699.0241855130016\n",
      "Current iteration=600, loss=1698.8678725896443\n",
      "Current iteration=700, loss=1698.7481565529874\n",
      "Current iteration=800, loss=1698.655354667586\n",
      "Current iteration=900, loss=1698.582753983257\n",
      "Current iteration=0, loss=1740.1201188295886\n",
      "Current iteration=100, loss=1738.644652155714\n",
      "Current iteration=200, loss=1738.3511241060278\n",
      "Current iteration=300, loss=1738.2329888893416\n",
      "Current iteration=400, loss=1738.1701250662607\n",
      "Current iteration=500, loss=1738.130265779897\n",
      "Current iteration=600, loss=1738.1021664284922\n",
      "Current iteration=700, loss=1738.081140290819\n",
      "Current iteration=800, loss=1738.0648895084335\n",
      "Current iteration=900, loss=1738.0521047407037\n",
      "Current iteration=0, loss=1717.9015503103717\n",
      "Current iteration=100, loss=1716.9455662468774\n",
      "Current iteration=200, loss=1716.664458688821\n",
      "Current iteration=300, loss=1716.537697009132\n",
      "Current iteration=400, loss=1716.4677590553172\n",
      "Current iteration=500, loss=1716.4239902214504\n",
      "Current iteration=600, loss=1716.3945472437522\n",
      "Current iteration=700, loss=1716.3739160374746\n",
      "Current iteration=800, loss=1716.3591030686316\n",
      "Current iteration=900, loss=1716.3482973801265\n",
      "Current iteration=0, loss=1704.7098720672705\n",
      "Current iteration=100, loss=1703.3802962133602\n",
      "Current iteration=200, loss=1702.9940636692686\n",
      "Current iteration=300, loss=1702.7774057739912\n",
      "Current iteration=400, loss=1702.6412251074103\n",
      "Current iteration=500, loss=1702.551121992453\n",
      "Current iteration=600, loss=1702.4895596616964\n",
      "Current iteration=700, loss=1702.4464990831702\n",
      "Current iteration=800, loss=1702.4158121514424\n",
      "Current iteration=900, loss=1702.3935975107952\n",
      "Current iteration=0, loss=1700.377135568498\n",
      "Current iteration=100, loss=1699.5951102020226\n",
      "Current iteration=200, loss=1699.2668948335759\n",
      "Current iteration=300, loss=1699.0952819820525\n",
      "Current iteration=400, loss=1698.9940562743745\n",
      "Current iteration=500, loss=1698.9286325807025\n",
      "Current iteration=600, loss=1698.8834170974205\n",
      "Current iteration=700, loss=1698.8506173448327\n",
      "Current iteration=800, loss=1698.825961513942\n",
      "Current iteration=900, loss=1698.806918234764\n",
      "Current iteration=0, loss=1682.6016417533106\n",
      "Current iteration=100, loss=1680.6068769523847\n",
      "Current iteration=200, loss=1679.976795490718\n",
      "Current iteration=300, loss=1679.714595064923\n",
      "Current iteration=400, loss=1679.5815929654482\n",
      "Current iteration=500, loss=1679.5031944621687\n",
      "Current iteration=600, loss=1679.4521259088147\n",
      "Current iteration=700, loss=1679.4167525332855\n",
      "Current iteration=800, loss=1679.391297597687\n",
      "Current iteration=900, loss=1679.3725062816088\n",
      "Current iteration=0, loss=1725.824919061332\n",
      "Current iteration=100, loss=1723.8614031109578\n",
      "Current iteration=200, loss=1723.4239524547586\n",
      "Current iteration=300, loss=1723.2724033328343\n",
      "Current iteration=400, loss=1723.208201386827\n",
      "Current iteration=500, loss=1723.1738790206778\n",
      "Current iteration=600, loss=1723.1516706891396\n",
      "Current iteration=700, loss=1723.1355260420091\n",
      "Current iteration=800, loss=1723.123061189715\n",
      "Current iteration=900, loss=1723.1131447875337\n",
      "Current iteration=0, loss=1708.4308040832707\n",
      "Current iteration=100, loss=1707.5506476869384\n",
      "Current iteration=200, loss=1707.3002587507092\n",
      "Current iteration=300, loss=1707.1984155952453\n",
      "Current iteration=400, loss=1707.1477933867454\n",
      "Current iteration=500, loss=1707.1191860174197\n",
      "Current iteration=600, loss=1707.1017584342353\n",
      "Current iteration=700, loss=1707.090678903714\n",
      "Current iteration=800, loss=1707.0834528511818\n",
      "Current iteration=900, loss=1707.078657897821\n",
      " Model logistic_regression \n",
      " Loss: 1708.7813383696289 \n",
      " Accuracy: 0.9117724867724867\n",
      "════════════════════════════════════════\n",
      "*The optimal hyperparameters*:\n",
      " Model: logistic_regression \n",
      " Optimal accuracy: 0.9117724867724867 \n",
      " Optimal lambda_: 0 \n",
      " Optimal gamma: 1e-05 \n",
      " \n",
      "┌──────────────────────────────────────┐\n",
      "│               Group: 4               │\n",
      "└──────────────────────────────────────┘\n",
      "────────────────────────────────────────\n",
      "lambda : 0,  gamma: 1e-05 \n",
      " \n",
      "Current iteration=0, loss=nan\n",
      "Current iteration=100, loss=60874.31592303685\n",
      "Current iteration=200, loss=60740.62126786858\n",
      "Current iteration=300, loss=60744.31039511252\n",
      "Current iteration=400, loss=60746.109562443344\n",
      "Current iteration=500, loss=60746.93147816627\n",
      "Current iteration=600, loss=60747.46052911944\n",
      "Current iteration=700, loss=60747.835946386665\n",
      "Current iteration=800, loss=60748.09976695665\n",
      "Current iteration=900, loss=60748.28116100644\n",
      "Current iteration=0, loss=60628.763189468846\n",
      "Current iteration=100, loss=60566.93439226356\n",
      "Current iteration=200, loss=60568.25812817231\n",
      "Current iteration=300, loss=60568.566526758186\n",
      "Current iteration=400, loss=60568.537297034985\n",
      "Current iteration=500, loss=60568.41744394641\n",
      "Current iteration=600, loss=60568.2901821758\n",
      "Current iteration=700, loss=60568.18165844371\n",
      "Current iteration=800, loss=60568.09695418826\n",
      "Current iteration=900, loss=60568.033785398846\n",
      "Current iteration=0, loss=60588.83734969736\n",
      "Current iteration=100, loss=60758.65904439336\n",
      "Current iteration=200, loss=60759.189318544224\n",
      "Current iteration=300, loss=60759.79498092113\n",
      "Current iteration=400, loss=60760.08257119399\n",
      "Current iteration=500, loss=60760.21912881816\n",
      "Current iteration=600, loss=60760.28804098317\n",
      "Current iteration=700, loss=60760.32543140687\n",
      "Current iteration=800, loss=60760.34717267505\n",
      "Current iteration=900, loss=60760.36055927817\n",
      "Current iteration=0, loss=60872.35817400195\n",
      "Current iteration=100, loss=60844.347087273054\n",
      "Current iteration=200, loss=60844.53158936034\n",
      "Current iteration=300, loss=60845.03588083876\n",
      "Current iteration=400, loss=60845.3147358638\n",
      "Current iteration=500, loss=60845.45549914301\n",
      "Current iteration=600, loss=60845.52489219719\n",
      "Current iteration=700, loss=60845.55847360187\n",
      "Current iteration=800, loss=60845.57442995466\n",
      "Current iteration=900, loss=60845.58186618337\n",
      "Current iteration=0, loss=60610.31447034434\n",
      "Current iteration=100, loss=60610.85823695738\n",
      "Current iteration=200, loss=60605.89927035307\n",
      "Current iteration=300, loss=60602.5286635936\n",
      "Current iteration=400, loss=60600.782661558536\n",
      "Current iteration=500, loss=60599.86965352871\n",
      "Current iteration=600, loss=60599.3744436184\n",
      "Current iteration=700, loss=60599.0942848313\n",
      "Current iteration=800, loss=60598.928916794546\n",
      "Current iteration=900, loss=60598.827414629995\n",
      "Current iteration=0, loss=60603.54138328531\n",
      "Current iteration=100, loss=60405.96281201059\n",
      "Current iteration=200, loss=60408.70820271039\n",
      "Current iteration=300, loss=60411.01654895127\n",
      "Current iteration=400, loss=60412.24932541404\n",
      "Current iteration=500, loss=60412.91466801899\n",
      "Current iteration=600, loss=60413.28661044066\n",
      "Current iteration=700, loss=60413.50277368287\n",
      "Current iteration=800, loss=60413.633307459335\n",
      "Current iteration=900, loss=60413.71489005976\n",
      "Current iteration=0, loss=60608.92108572775\n",
      "Current iteration=100, loss=61007.00179161732\n",
      "Current iteration=200, loss=61002.408579733645\n",
      "Current iteration=300, loss=61000.048047556076\n",
      "Current iteration=400, loss=60998.94195577516\n",
      "Current iteration=500, loss=60998.395616633614\n",
      "Current iteration=600, loss=60998.111818578574\n",
      "Current iteration=700, loss=60997.957414528166\n",
      "Current iteration=800, loss=60997.86958202122\n",
      "Current iteration=900, loss=60997.81747688447\n",
      "Current iteration=0, loss=60747.674340211524\n",
      "Current iteration=100, loss=60433.118284225886\n",
      "Current iteration=200, loss=60436.83282061452\n",
      "Current iteration=300, loss=60438.499299421535\n",
      "Current iteration=400, loss=60439.22757051077\n",
      "Current iteration=500, loss=60439.55999315413\n",
      "Current iteration=600, loss=60439.71693514409\n",
      "Current iteration=700, loss=60439.79273322623\n",
      "Current iteration=800, loss=60439.83003237924\n",
      "Current iteration=900, loss=60439.84875255642\n",
      "Current iteration=0, loss=60683.112093709286\n",
      "Current iteration=100, loss=60730.82718033259\n",
      "Current iteration=200, loss=60725.17727962204\n",
      "Current iteration=300, loss=60724.03414268939\n",
      "Current iteration=400, loss=60723.720329313684\n",
      "Current iteration=500, loss=60723.63885685752\n",
      "Current iteration=600, loss=60723.63049803079\n",
      "Current iteration=700, loss=60723.644018385414\n",
      "Current iteration=800, loss=60723.66178325393\n",
      "Current iteration=900, loss=60723.67792106326\n",
      "Current iteration=0, loss=60886.811320982044\n",
      "Current iteration=100, loss=60881.537982582086\n",
      "Current iteration=200, loss=60882.982421359935\n",
      "Current iteration=300, loss=60882.948451719436\n",
      "Current iteration=400, loss=60882.846806414644\n",
      "Current iteration=500, loss=60882.74309433842\n",
      "Current iteration=600, loss=60882.65319830293\n",
      "Current iteration=700, loss=60882.58205186029\n",
      "Current iteration=800, loss=60882.52865352174\n",
      "Current iteration=900, loss=60882.489799571005\n",
      " Model logistic_regression \n",
      " Loss: 58432.747113292964 \n",
      " Accuracy: 0.6178387901923358\n",
      "════════════════════════════════════════\n",
      "*The optimal hyperparameters*:\n",
      " Model: logistic_regression \n",
      " Optimal accuracy: 0.6178387901923358 \n",
      " Optimal lambda_: 0 \n",
      " Optimal gamma: 1e-05 \n",
      " \n",
      "┌──────────────────────────────────────┐\n",
      "│               Group: 5               │\n",
      "└──────────────────────────────────────┘\n",
      "────────────────────────────────────────\n",
      "lambda : 0,  gamma: 1e-05 \n",
      " \n",
      "Current iteration=0, loss=nan\n",
      "Current iteration=100, loss=3620.779100705038\n",
      "Current iteration=200, loss=2417.591506803961\n",
      "Current iteration=300, loss=1906.3166022309342\n",
      "Current iteration=400, loss=1646.0012927162766\n",
      "Current iteration=500, loss=1485.596547670869\n",
      "Current iteration=600, loss=1378.0574644873927\n",
      "Current iteration=700, loss=1303.82910489921\n",
      "Current iteration=800, loss=1251.7067435497734\n",
      "Current iteration=900, loss=1214.4623619235465\n",
      "Current iteration=0, loss=1154.5867701951072\n",
      "Current iteration=100, loss=1134.6181802326016\n",
      "Current iteration=200, loss=1120.3524191602326\n",
      "Current iteration=300, loss=1109.687263382731\n",
      "Current iteration=400, loss=1101.6271458720212\n",
      "Current iteration=500, loss=1095.4724371797477\n",
      "Current iteration=600, loss=1090.7174126515129\n",
      "Current iteration=700, loss=1086.995459365592\n",
      "Current iteration=800, loss=1084.0409525388625\n",
      "Current iteration=900, loss=1081.6613585989528\n",
      "Current iteration=0, loss=1092.493399017837\n",
      "Current iteration=100, loss=1089.6354527651972\n",
      "Current iteration=200, loss=1087.8337013504472\n",
      "Current iteration=300, loss=1086.4487978956163\n",
      "Current iteration=400, loss=1085.329133967644\n",
      "Current iteration=500, loss=1084.39532648407\n",
      "Current iteration=600, loss=1083.599369715963\n",
      "Current iteration=700, loss=1082.9101849659394\n",
      "Current iteration=800, loss=1082.3066012332642\n",
      "Current iteration=900, loss=1081.773535106801\n",
      "Current iteration=0, loss=1071.4773851982318\n",
      "Current iteration=100, loss=1069.696017885839\n",
      "Current iteration=200, loss=1068.694492975554\n",
      "Current iteration=300, loss=1067.9714720649026\n",
      "Current iteration=400, loss=1067.4067880744728\n",
      "Current iteration=500, loss=1066.9436327020658\n",
      "Current iteration=600, loss=1066.5511117660772\n",
      "Current iteration=700, loss=1066.2110396376688\n",
      "Current iteration=800, loss=1065.911984820088\n",
      "Current iteration=900, loss=1065.6463192290385\n",
      "Current iteration=0, loss=1090.9315821996047\n",
      "Current iteration=100, loss=1089.4911799780064\n",
      "Current iteration=200, loss=1088.878240789627\n",
      "Current iteration=300, loss=1088.5174006214597\n",
      "Current iteration=400, loss=1088.2815932773458\n",
      "Current iteration=500, loss=1088.1154839526585\n",
      "Current iteration=600, loss=1087.9916125615455\n",
      "Current iteration=700, loss=1087.8952573842814\n",
      "Current iteration=800, loss=1087.8179387389523\n",
      "Current iteration=900, loss=1087.7544381027046\n",
      "Current iteration=0, loss=1087.637099184205\n",
      "Current iteration=100, loss=1086.850429986943\n",
      "Current iteration=200, loss=1086.3742276203645\n",
      "Current iteration=300, loss=1086.03094785121\n",
      "Current iteration=400, loss=1085.7646782600357\n",
      "Current iteration=500, loss=1085.5486997860435\n",
      "Current iteration=600, loss=1085.368272346896\n",
      "Current iteration=700, loss=1085.2144670281045\n",
      "Current iteration=800, loss=1085.0814748469475\n",
      "Current iteration=900, loss=1084.9652934665041\n",
      "Current iteration=0, loss=1090.0704931137543\n",
      "Current iteration=100, loss=1089.307931877754\n",
      "Current iteration=200, loss=1088.877635882314\n",
      "Current iteration=300, loss=1088.6029336621993\n",
      "Current iteration=400, loss=1088.4127245907503\n",
      "Current iteration=500, loss=1088.2724167475499\n",
      "Current iteration=600, loss=1088.1638101110016\n",
      "Current iteration=700, loss=1088.0766003165254\n",
      "Current iteration=800, loss=1088.0045610693896\n",
      "Current iteration=900, loss=1087.9437184832034\n",
      "Current iteration=0, loss=1081.1418424234344\n",
      "Current iteration=100, loss=1079.9810372213715\n",
      "Current iteration=200, loss=1079.4212742990471\n",
      "Current iteration=300, loss=1079.1026791018044\n",
      "Current iteration=400, loss=1078.9040596781922\n",
      "Current iteration=500, loss=1078.7707493937814\n",
      "Current iteration=600, loss=1078.6756937602177\n",
      "Current iteration=700, loss=1078.604473648679\n",
      "Current iteration=800, loss=1078.5489134241104\n",
      "Current iteration=900, loss=1078.5041302354111\n",
      "Current iteration=0, loss=1076.1828807062952\n",
      "Current iteration=100, loss=1075.4309907303013\n",
      "Current iteration=200, loss=1075.077047456442\n",
      "Current iteration=300, loss=1074.8724695248852\n",
      "Current iteration=400, loss=1074.7408301174414\n",
      "Current iteration=500, loss=1074.6490220329915\n",
      "Current iteration=600, loss=1074.5810325060706\n",
      "Current iteration=700, loss=1074.5283749583816\n",
      "Current iteration=800, loss=1074.486192190732\n",
      "Current iteration=900, loss=1074.451518536732\n",
      "Current iteration=0, loss=1046.107700102739\n",
      "Current iteration=100, loss=1044.1542882771337\n",
      "Current iteration=200, loss=1043.2889932119538\n",
      "Current iteration=300, loss=1042.7860738464146\n",
      "Current iteration=400, loss=1042.4664077715156\n",
      "Current iteration=500, loss=1042.251452835356\n",
      "Current iteration=600, loss=1042.1013665839537\n",
      "Current iteration=700, loss=1041.993779429033\n",
      "Current iteration=800, loss=1041.9151304183692\n",
      "Current iteration=900, loss=1041.8567248871223\n",
      " Model logistic_regression \n",
      " Loss: 1086.9247724976287 \n",
      " Accuracy: 0.9006787330316743\n",
      "════════════════════════════════════════\n",
      "*The optimal hyperparameters*:\n",
      " Model: logistic_regression \n",
      " Optimal accuracy: 0.9006787330316743 \n",
      " Optimal lambda_: 0 \n",
      " Optimal gamma: 1e-05 \n",
      " \n",
      "\n",
      " * Overall Accuracy = 0.7154\n"
     ]
    }
   ],
   "source": [
    "lambda_ = [0]\n",
    "gamma_ = [1e-5]\n",
    "accuracy_ = []\n",
    "group_size_ = []\n",
    "for idx_data,data in enumerate(data_pro):\n",
    "    print('┌'+'─' * 38+'┐')\n",
    "    print('│               Group: %i               │'%idx_data)\n",
    "    print('└'+'─' * 38+'┘')\n",
    "    tx = data[0]\n",
    "    tx = np.hstack((tx**0 ,tx))\n",
    "    y = data[1]\n",
    "    initial_w_ = np.ones(tx.shape[1])\n",
    "    optimal_accuracy, optimal_lambda_, optimal_gamma = Hyperparameter_optimization( y, tx,k_fold=10, \n",
    "                                                                                   lambdas=lambda_ , \n",
    "                                                                                   gammas = gamma_, \n",
    "                                                                                   initial_w = initial_w_, \n",
    "                                                                                   max_iters = 1000, \n",
    "                                                                                   model = 'logistic_regression')\n",
    "    accuracy_.append(optimal_accuracy)\n",
    "    group_size_.append(len(data[2]))\n",
    "\n",
    "accuracy_all = np.dot(accuracy_,group_size_)/np.sum(group_size_)\n",
    "print('\\n * Overall Accuracy = %.4f'%accuracy_all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
